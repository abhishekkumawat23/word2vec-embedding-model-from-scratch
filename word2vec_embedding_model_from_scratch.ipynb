{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv9RUPOxBmUNLH7ljVMovw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekkumawat23/word2vec-embedding-model-from-scratch/blob/main/word2vec_embedding_model_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec Embedding Model from Scratch\n",
        "\n",
        "In this notebook, we create a simple version of the word2vec embedding model from scratch.\n",
        "The aim is educational - this contains simplifications and is not fully optimized.\n",
        "\n",
        "## Important Context\n",
        "\n",
        "Modern LLMs learn/train their embeddings along with the main LLM training itself, so they\n",
        "don't need a separate embedding model. When you train an LLM, you simultaneously train:\n",
        "1. The model to predict/generate the next token (main objective)\n",
        "2. The embedding vectors for the vocabulary\n",
        "\n",
        "For non-LLM cases or pre-trained embeddings, we implement word2vec here. Other alternatives\n",
        "exist (like GloVe), but we focus on word2vec with skip-gram and negative sampling.\n",
        "\n",
        "## Algorithm Overview\n",
        "\n",
        "**Core Task:**\n",
        "Word2vec is a binary classification task that takes a pair of words as input and predicts\n",
        "whether they belong to the same context. Instead of returning discrete 0 or 1, it outputs\n",
        "the probability that the pair shares the same context.\n",
        "\n",
        "**Creating Training Pairs:**\n",
        "\n",
        "1. **Positive Pairs (Skip-gram approach):**\n",
        "   - Use a sliding window of fixed size over the text corpus\n",
        "   - For each window, the center word becomes the first token\n",
        "   - All other words in the window become second tokens\n",
        "   - Create pairs: (center_word, context_word) with label 1\n",
        "   - Example: \"the quick brown fox\" with window_size=2\n",
        "     - Center=\"brown\" → pairs: (brown, the), (brown, quick), (brown, fox)\n",
        "\n",
        "2. **Negative Pairs (Negative sampling):**\n",
        "   - Keep the same center word from the sliding window\n",
        "   - Randomly sample words from the entire vocabulary as second tokens\n",
        "   - These become negative pairs with label 0\n",
        "   - Risk: Might accidentally sample an actual context word, but probability is low\n",
        "   - Typically sample 5-20 negative pairs per positive pair\n",
        "\n",
        "**Training Input:**\n",
        "For each center word, we provide:\n",
        "- 1 positive pair (actual context word) with target = 1\n",
        "- N negative pairs (random words) with target = 0\n",
        "\n",
        "**Model Architecture (Shallow - No Hidden Layers):**\n",
        "\n",
        "1. **Two Embedding Matrices:**\n",
        "   - Center word embeddings: [vocab_size × embedding_dim]\n",
        "   - Context word embeddings: [vocab_size × embedding_dim]\n",
        "   - Each word has two representations (one as center, one as context)\n",
        "\n",
        "2. **Forward Pass:**\n",
        "   - Lookup embeddings for center word and context word\n",
        "   - Compute dot product: similarity = center_embed · context_embed\n",
        "   - Dot product measures semantic similarity between embeddings\n",
        "   - Apply sigmoid activation: probability = 1 / (1 + e^(-similarity))\n",
        "\n",
        "3. **Why Sigmoid?**\n",
        "   - Introduces non-linearity (enables learning complex semantic relationships)\n",
        "   - Maps output to [0, 1] range (perfect for probability)\n",
        "   - Output represents: P(words are in same context)\n",
        "\n",
        "4. **Loss and Optimization:**\n",
        "   - Binary Cross-Entropy Loss: measures difference between predicted probability and target\n",
        "   - Adam optimizer: updates embedding matrices to minimize loss\n",
        "   - Alternative: SGD or other optimizers work too\n",
        "\n",
        "**Final Embeddings:**\n",
        "After training, average the center and context embeddings to get the final word vectors:\n",
        "- final_embedding[word] = (center_embedding[word] + context_embedding[word]) / 2\n",
        "- This captures both perspectives and typically gives better results\n",
        "\n",
        "**Key Insight:**\n",
        "No neural network layers needed! The embeddings themselves are the parameters being learned.\n",
        "Through the training objective, words that appear in similar contexts naturally end up with\n",
        "similar embedding vectors."
      ],
      "metadata": {
        "id": "2NPzMaITQ6Ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [WARNING] Generated by Claude\n",
        "\n",
        "Below code is only for reference to be used when I actually implement the word2vec. Below code is generated by claude when i asked to create a simple version of word2vec. Remove it once you have implemented the word2vec by yourself."
      ],
      "metadata": {
        "id": "WQuSm6hHxdpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Word2Vec implementation with Skip-gram and Negative Sampling\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "\n",
        "        # Center word embeddings (input)\n",
        "        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Context word embeddings (output)\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Initialize embeddings with small random values\n",
        "        self.center_embeddings.weight.data.uniform_(-0.5/embedding_dim, 0.5/embedding_dim)\n",
        "        self.context_embeddings.weight.data.uniform_(-0.5/embedding_dim, 0.5/embedding_dim)\n",
        "\n",
        "    def forward(self, center_words, context_words):\n",
        "        \"\"\"\n",
        "        Forward pass: compute probability that word pairs are in same context\n",
        "\n",
        "        Args:\n",
        "            center_words: [batch_size] - indices of center words\n",
        "            context_words: [batch_size, num_samples] - indices of context/negative words\n",
        "                          (first sample is positive, rest are negative)\n",
        "\n",
        "        Returns:\n",
        "            [batch_size, num_samples] - probabilities for each pair\n",
        "        \"\"\"\n",
        "        # Get center word embeddings: [batch_size, embedding_dim]\n",
        "        center_embeds = self.center_embeddings(center_words)\n",
        "\n",
        "        # Get context word embeddings: [batch_size, num_samples, embedding_dim]\n",
        "        context_embeds = self.context_embeddings(context_words)\n",
        "\n",
        "        # Compute dot product (similarity measure): [batch_size, num_samples]\n",
        "        # Using einsum for clarity: 'be' (batch, embed) × 'bse' (batch, samples, embed) -> 'bs'\n",
        "        scores = torch.einsum('be,bse->bs', center_embeds, context_embeds)\n",
        "\n",
        "        # Apply sigmoid to convert to probability [0, 1]\n",
        "        # High score (similar embeddings) → probability near 1\n",
        "        # Low score (dissimilar embeddings) → probability near 0\n",
        "        probs = torch.sigmoid(scores)\n",
        "\n",
        "        return probs\n",
        "\n",
        "\n",
        "class Word2VecTrainer:\n",
        "    \"\"\"\n",
        "    Trainer for Word2Vec model with data preparation and training loop\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim=100, window_size=5, num_negative_samples=5,\n",
        "                 min_count=5, learning_rate=0.025):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.window_size = window_size\n",
        "        self.num_negative_samples = num_negative_samples\n",
        "        self.min_count = min_count\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.word_counts = None\n",
        "        self.model = None\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        \"\"\"Build vocabulary from sentences\"\"\"\n",
        "        # Count word frequencies\n",
        "        word_counts = Counter()\n",
        "        for sentence in sentences:\n",
        "            word_counts.update(sentence.lower().split())\n",
        "\n",
        "        # Filter by min_count\n",
        "        word_counts = {word: count for word, count in word_counts.items()\n",
        "                      if count >= self.min_count}\n",
        "\n",
        "        # Create word-to-index mapping\n",
        "        self.word2idx = {word: idx for idx, word in enumerate(word_counts.keys())}\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "\n",
        "        # Store word counts for negative sampling (raised to 0.75 power)\n",
        "        self.word_counts = np.array([word_counts[self.idx2word[i]]\n",
        "                                     for i in range(len(self.word2idx))], dtype=np.float64)\n",
        "        self.word_counts = np.power(self.word_counts, 0.75)\n",
        "        self.word_counts /= self.word_counts.sum()\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
        "\n",
        "        return len(self.word2idx)\n",
        "\n",
        "    def generate_training_data(self, sentences, batch_size=512):\n",
        "        \"\"\"\n",
        "        Generate training batches with positive and negative samples\n",
        "\n",
        "        For each center word in a sliding window:\n",
        "        - Create 1 positive pair with actual context word (label=1)\n",
        "        - Create N negative pairs with random words (label=0)\n",
        "\n",
        "        Returns batches of:\n",
        "        - center_words: [batch_size]\n",
        "        - context_words: [batch_size, 1 + num_negative_samples]\n",
        "        - labels: [batch_size, 1 + num_negative_samples]\n",
        "        \"\"\"\n",
        "        for sentence in sentences:\n",
        "            words = sentence.lower().split()\n",
        "            word_indices = [self.word2idx[w] for w in words if w in self.word2idx]\n",
        "\n",
        "            if len(word_indices) < 2:\n",
        "                continue\n",
        "\n",
        "            # Generate skip-gram pairs using sliding window\n",
        "            for center_pos, center_word in enumerate(word_indices):\n",
        "                # Define context window around center word\n",
        "                start = max(0, center_pos - self.window_size)\n",
        "                end = min(len(word_indices), center_pos + self.window_size + 1)\n",
        "\n",
        "                # For each word in the window (except center itself)\n",
        "                for context_pos in range(start, end):\n",
        "                    if context_pos == center_pos:\n",
        "                        continue\n",
        "\n",
        "                    context_word = word_indices[context_pos]\n",
        "\n",
        "                    # Create POSITIVE sample (words that actually appear together)\n",
        "                    center_batch = [center_word]\n",
        "                    context_batch = [[context_word]]  # First sample is positive\n",
        "                    label_batch = [[1.0]]  # Label 1 = same context\n",
        "\n",
        "                    # Add NEGATIVE samples (random words from vocabulary)\n",
        "                    # Sampled with frequency^0.75 to give rare words better chance\n",
        "                    negative_samples = np.random.choice(\n",
        "                        len(self.word2idx),\n",
        "                        size=self.num_negative_samples,\n",
        "                        replace=False,\n",
        "                        p=self.word_counts  # Weighted by word frequency^0.75\n",
        "                    )\n",
        "\n",
        "                    # Note: Small chance negative sample is actually a context word\n",
        "                    # but probability is low enough to ignore\n",
        "                    context_batch[0].extend(negative_samples.tolist())\n",
        "                    label_batch[0].extend([0.0] * self.num_negative_samples)\n",
        "\n",
        "                    yield (\n",
        "                        torch.LongTensor(center_batch),\n",
        "                        torch.LongTensor(context_batch),\n",
        "                        torch.FloatTensor(label_batch)\n",
        "                    )\n",
        "\n",
        "    def train(self, sentences, epochs=5):\n",
        "        \"\"\"Train the Word2Vec model\"\"\"\n",
        "        vocab_size = self.build_vocab(sentences)\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = Word2Vec(vocab_size, self.embedding_dim)\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Binary Cross-Entropy Loss for binary classification (same context or not)\n",
        "        # Loss = -[y*log(p) + (1-y)*log(1-p)] where y is target, p is predicted probability\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            batch_count = 0\n",
        "\n",
        "            for center, context, labels in self.generate_training_data(sentences):\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass: get probabilities for each pair\n",
        "                probs = self.model(center, context)\n",
        "\n",
        "                # Compute loss between predicted probabilities and true labels\n",
        "                loss = criterion(probs, labels)\n",
        "\n",
        "                # Backward pass: update embedding matrices\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                batch_count += 1\n",
        "\n",
        "            avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def get_word_vector(self, word):\n",
        "        \"\"\"\n",
        "        Get embedding vector for a word by averaging center and context embeddings\n",
        "\n",
        "        Args:\n",
        "            word: The word to get embedding for\n",
        "        \"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            return None\n",
        "        idx = self.word2idx[word]\n",
        "\n",
        "        # Average both embedding matrices for best results\n",
        "        center_vec = self.model.center_embeddings.weight[idx].detach().numpy()\n",
        "        context_vec = self.model.context_embeddings.weight[idx].detach().numpy()\n",
        "        return (center_vec + context_vec) / 2\n",
        "\n",
        "    def get_embedding_matrix(self):\n",
        "        \"\"\"\n",
        "        Get the final embedding matrix [vocab_size x embedding_dim]\n",
        "\n",
        "        This is what you'd save and use for downstream tasks.\n",
        "        Averages center and context embeddings for best representation.\n",
        "\n",
        "        Why average both matrices?\n",
        "        - Center embeddings learn: \"when I'm the query word\"\n",
        "        - Context embeddings learn: \"when I'm the context word\"\n",
        "        - Both capture similar semantic relationships but from different perspectives\n",
        "        - Averaging combines both views for richer representations\n",
        "        \"\"\"\n",
        "        center_matrix = self.model.center_embeddings.weight.detach().numpy()\n",
        "        context_matrix = self.model.context_embeddings.weight.detach().numpy()\n",
        "        return (center_matrix + context_matrix) / 2\n",
        "\n",
        "    def find_similar_words(self, word, top_k=5):\n",
        "        \"\"\"Find most similar words using cosine similarity\"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            return []\n",
        "\n",
        "        word_vec = self.get_word_vector(word)\n",
        "\n",
        "        # Get all word embeddings\n",
        "        all_embeddings = self.model.center_embeddings.weight.detach().numpy()\n",
        "\n",
        "        # Compute cosine similarities\n",
        "        similarities = np.dot(all_embeddings, word_vec) / (\n",
        "            np.linalg.norm(all_embeddings, axis=1) * np.linalg.norm(word_vec)\n",
        "        )\n",
        "\n",
        "        # Get top-k indices (excluding the word itself)\n",
        "        similar_indices = similarities.argsort()[::-1][1:top_k+1]\n",
        "\n",
        "        return [(self.idx2word[idx], similarities[idx]) for idx in similar_indices]\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample corpus\n",
        "    sentences = [\n",
        "        \"the quick brown fox jumps over the lazy dog\",\n",
        "        \"the dog is very lazy and sleeps all day\",\n",
        "        \"the cat is quick and agile\",\n",
        "        \"a quick brown fox runs through the forest\",\n",
        "        \"the lazy cat sleeps on the couch\",\n",
        "        \"dogs and cats are great pets\",\n",
        "        \"the fox is clever and quick\",\n",
        "        \"brown bears live in the forest\"\n",
        "    ] * 20  # Repeat for more training data\n",
        "\n",
        "    # Initialize and train\n",
        "    trainer = Word2VecTrainer(\n",
        "        embedding_dim=50,\n",
        "        window_size=2,\n",
        "        num_negative_samples=5,\n",
        "        min_count=2,\n",
        "        learning_rate=0.01\n",
        "    )\n",
        "\n",
        "    print(\"Training Word2Vec model...\")\n",
        "    trainer.train(sentences, epochs=10)\n",
        "\n",
        "    # Test the model\n",
        "    print(\"\\nWord vectors learned!\")\n",
        "\n",
        "    # Get the final embedding matrix (averaged from both center and context)\n",
        "    embedding_matrix = trainer.get_embedding_matrix()\n",
        "    print(f\"\\nFinal embedding matrix shape: {embedding_matrix.shape}\")\n",
        "    print(f\"(vocab_size={len(trainer.word2idx)}, embedding_dim={trainer.embedding_dim})\")\n",
        "\n",
        "    print(\"\\nSimilar words to 'dog':\")\n",
        "    similar = trainer.find_similar_words('dog', top_k=3)\n",
        "    for word, similarity in similar:\n",
        "        print(f\"  {word}: {similarity:.4f}\")\n",
        "\n",
        "    print(\"\\nSimilar words to 'quick':\")\n",
        "    similar = trainer.find_similar_words('quick', top_k=3)\n",
        "    for word, similarity in similar:\n",
        "        print(f\"  {word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moYtypSqyMPF",
        "outputId": "6b804c7c-4276-4821-e5c3-a29fd4d52169"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Word2Vec model...\n",
            "Vocabulary size: 31\n",
            "Epoch 1/10, Average Loss: 0.4063\n",
            "Epoch 2/10, Average Loss: 0.3408\n",
            "Epoch 3/10, Average Loss: 0.3284\n",
            "Epoch 4/10, Average Loss: 0.3226\n",
            "Epoch 5/10, Average Loss: 0.3156\n",
            "Epoch 6/10, Average Loss: 0.3132\n",
            "Epoch 7/10, Average Loss: 0.3094\n",
            "Epoch 8/10, Average Loss: 0.3094\n",
            "Epoch 9/10, Average Loss: 0.3121\n",
            "Epoch 10/10, Average Loss: 0.3092\n",
            "\n",
            "Word vectors learned!\n",
            "\n",
            "Final embedding matrix shape: (31, 50)\n",
            "(vocab_size=31, embedding_dim=50)\n",
            "\n",
            "Similar words to 'dog':\n",
            "  lazy: 0.4625\n",
            "  very: 0.3182\n",
            "  is: 0.2009\n",
            "\n",
            "Similar words to 'quick':\n",
            "  quick: 0.2955\n",
            "  a: 0.2879\n",
            "  agile: 0.2383\n"
          ]
        }
      ]
    }
  ]
}